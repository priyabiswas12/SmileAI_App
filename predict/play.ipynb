{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.meshsegnet import *\n",
    "import vedo\n",
    "from vedo import *\n",
    "import pandas as pd\n",
    "from utils.loss_and_metrics import *\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy.io as sio\n",
    "import shutil\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pygco import cut_from_graph\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_labels(self, model_name, ip_path, op_path, file_num,downsample):\n",
    "\n",
    "    \n",
    "        gpu_id = 0\n",
    "        #torch.cuda.set_device(gpu_id) # assign which gpu will be used (only linux works)\n",
    "        upsampling_method = 'KNN'\n",
    "\n",
    "        model_path = './models'\n",
    "        model_name =model_name\n",
    "\n",
    "        mesh_path = ip_path # need to modify\n",
    "        #sample_filenames = ['{}.stl'.format(i) for i in range(2,len(os.listdir(ip_path)) + 1)] # need to modify\n",
    "        #i_sample='{}.stl'.format(file_num)\n",
    "        i_sample=file_num\n",
    "        print(i_sample)\n",
    "        \n",
    "\n",
    "        output_path = op_path\n",
    "        if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "\n",
    "        num_classes = 17\n",
    "        num_channels = 15\n",
    "\n",
    "        # set model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = MeshSegNet(num_classes=num_classes, num_channels=num_channels).to(device, dtype=torch.float)\n",
    "\n",
    "        # load trained model\n",
    "        checkpoint = torch.load(os.path.join(model_path, model_name), map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        del checkpoint\n",
    "        model = model.to(device, dtype=torch.float)\n",
    "\n",
    "        #cudnn\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "\n",
    "        # Predicting\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            #for i_sample in sample_filenames:\n",
    "\n",
    "            start_time = time.time()\n",
    "            # create tmp folder\n",
    "            tmp_path = './.tmp/'\n",
    "            if not os.path.exists(tmp_path):\n",
    "                os.makedirs(tmp_path)\n",
    "\n",
    "            print('Predicting Sample filename: {}'.format(i_sample))\n",
    "            # read image and label (annotation)\n",
    "            mesh = vedo.load(os.path.join(mesh_path, i_sample))\n",
    "            mesh_d = mesh.clone()\n",
    "\n",
    "            # pre-processing: downsampling\n",
    "            if downsample:\n",
    "                print('\\tDownsampling...')\n",
    "                target_num = 10000\n",
    "                ratio = target_num/mesh.ncells # calculate ratio\n",
    "                mesh_d.decimate(fraction=ratio)\n",
    "                mesh_d.clean() #remove duplicate vertices etc\n",
    "            predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32)\n",
    "\n",
    "            # move mesh to origin\n",
    "            print('\\tPredicting...')\n",
    "            cells = np.zeros([mesh_d.ncells, 9], dtype='float32')\n",
    "            for i in range(len(cells)):\n",
    "                cells[i][0], cells[i][1], cells[i][2] = mesh_d.dataset.GetPoint(mesh_d.dataset.GetCell(i).GetPointId(0)) # don't need to copy\n",
    "                cells[i][3], cells[i][4], cells[i][5] = mesh_d.dataset.GetPoint(mesh_d.dataset.GetCell(i).GetPointId(1)) # don't need to copy\n",
    "                cells[i][6], cells[i][7], cells[i][8] = mesh_d.dataset.GetPoint(mesh_d.dataset.GetCell(i).GetPointId(2)) # don't need to copy\n",
    "\n",
    "            original_cells_d = cells.copy()\n",
    "\n",
    "            mean_cell_centers = mesh_d.center_of_mass()\n",
    "            cells[:, 0:3] -= mean_cell_centers[0:3]\n",
    "            cells[:, 3:6] -= mean_cell_centers[0:3]\n",
    "            cells[:, 6:9] -= mean_cell_centers[0:3]\n",
    "\n",
    "            # customized normal calculation; the vtk/vedo build-in function will change number of points\n",
    "            v1 = np.zeros([mesh_d.ncells, 3], dtype='float32')\n",
    "            v2 = np.zeros([mesh_d.ncells, 3], dtype='float32')\n",
    "            v1[:, 0] = cells[:, 0] - cells[:, 3]\n",
    "            v1[:, 1] = cells[:, 1] - cells[:, 4]\n",
    "            v1[:, 2] = cells[:, 2] - cells[:, 5]\n",
    "            v2[:, 0] = cells[:, 3] - cells[:, 6]\n",
    "            v2[:, 1] = cells[:, 4] - cells[:, 7]\n",
    "            v2[:, 2] = cells[:, 5] - cells[:, 8]\n",
    "            mesh_normals = np.cross(v1, v2)\n",
    "            mesh_normal_length = np.linalg.norm(mesh_normals, axis=1)\n",
    "\n",
    "            mesh_normals[:, 0]= np.divide(mesh_normals[:, 0], mesh_normal_length[:], out=np.zeros_like(mesh_normals[:, 0]), where=mesh_normal_length[:]!=0)\n",
    "            mesh_normals[:, 1]= np.divide(mesh_normals[:, 1], mesh_normal_length[:], out=np.zeros_like(mesh_normals[:, 1]), where=mesh_normal_length[:]!=0)\n",
    "            mesh_normals[:, 2]= np.divide(mesh_normals[:, 2], mesh_normal_length[:], out=np.zeros_like(mesh_normals[:, 2]), where=mesh_normal_length[:]!=0)\n",
    "            #mesh_normals[:, 0] /= mesh_normal_length[:]\n",
    "            #mesh_normals[:, 1] /= mesh_normal_length[:]\n",
    "            #mesh_normals[:, 2] /= mesh_normal_length[:]\n",
    "            mesh_d.celldata['Normal']=mesh_normals\n",
    "\n",
    "            # preprae input\n",
    "            points = mesh_d.vertices.copy()\n",
    "            points[:, 0:3] -= mean_cell_centers[0:3]\n",
    "            normals = mesh_d.celldata['Normal'].copy() # need to copy, they use the same memory address\n",
    "            #barycenters = mesh_d.cell_centers()\n",
    "            barycenters = mesh_d.cell_centers().vertices\n",
    "            #barycenters =np.array(mesh_d.cell_centers()) # don't need to copy\n",
    "            barycenters -= mean_cell_centers[0:3]\n",
    "\n",
    "            #normalized data\n",
    "            maxs = points.max(axis=0)\n",
    "            mins = points.min(axis=0)\n",
    "            means = points.mean(axis=0)\n",
    "            stds = points.std(axis=0)\n",
    "            nmeans = normals.mean(axis=0)\n",
    "            nstds = normals.std(axis=0)\n",
    "\n",
    "            for i in range(3):\n",
    "                cells[:, i] = (cells[:, i] - means[i]) / stds[i] #point 1\n",
    "                cells[:, i+3] = (cells[:, i+3] - means[i]) / stds[i] #point 2\n",
    "                cells[:, i+6] = (cells[:, i+6] - means[i]) / stds[i] #point 3\n",
    "                barycenters[:,i] = (barycenters[:,i] - mins[i]) / (maxs[i]-mins[i])\n",
    "                normals[:,i] = (normals[:,i] - nmeans[i]) / nstds[i]\n",
    "\n",
    "            X = np.column_stack((cells, barycenters, normals))\n",
    "\n",
    "            # computing A_S and A_L\n",
    "            A_S = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n",
    "            A_L = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n",
    "            D = distance_matrix(X[:, 9:12], X[:, 9:12])\n",
    "            A_S[D<0.1] = 1.0\n",
    "            A_S = A_S / np.dot(np.sum(A_S, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n",
    "\n",
    "            A_L[D<0.2] = 1.0\n",
    "            A_L = A_L / np.dot(np.sum(A_L, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n",
    "\n",
    "            # numpy -> torch.tensor\n",
    "            X = X.transpose(1, 0)\n",
    "            X = X.reshape([1, X.shape[0], X.shape[1]])\n",
    "            X = torch.from_numpy(X).to(device, dtype=torch.float)\n",
    "            A_S = A_S.reshape([1, A_S.shape[0], A_S.shape[1]])\n",
    "            A_L = A_L.reshape([1, A_L.shape[0], A_L.shape[1]])\n",
    "            A_S = torch.from_numpy(A_S).to(device, dtype=torch.float)\n",
    "            A_L = torch.from_numpy(A_L).to(device, dtype=torch.float)\n",
    "\n",
    "            tensor_prob_output = model(X, A_S, A_L).to(device, dtype=torch.float)\n",
    "            patch_prob_output = tensor_prob_output.cpu().numpy()\n",
    "\n",
    "            for i_label in range(num_classes):\n",
    "                predicted_labels_d[np.argmax(patch_prob_output[0, :], axis=-1)==i_label] = i_label\n",
    "\n",
    "            # output downsampled predicted labels\n",
    "            #mesh2 = mesh_d.clone()\n",
    "            #mesh2.celldata['Label']=predicted_labels_d\n",
    "            #vedo.write(mesh2, os.path.join(output_path, '{}_d_predicted.vtp'.format(i_sample[:-4])))\n",
    "\n",
    "            # refinement\n",
    "            print('\\tRefining by pygco...')\n",
    "            round_factor = 100\n",
    "            patch_prob_output[patch_prob_output<1.0e-6] = 1.0e-6\n",
    "\n",
    "            # unaries\n",
    "            unaries = -round_factor * np.log10(patch_prob_output)\n",
    "            unaries = unaries.astype(np.int32)\n",
    "            unaries = unaries.reshape(-1, num_classes)\n",
    "\n",
    "            # parawise\n",
    "            pairwise = (1 - np.eye(num_classes, dtype=np.int32))\n",
    "\n",
    "            #edges\n",
    "            normals = mesh_d.celldata['Normal'].copy() # need to copy, they use the same memory address\n",
    "            cells = original_cells_d.copy()\n",
    "            #barycenters = mesh_d.cell_centers # don't need to copy\n",
    "            #barycenters =np.array(mesh_d.cell_centers())\n",
    "            barycenters = mesh_d.cell_centers().vertices\n",
    "            cell_ids = np.asarray(mesh_d.cells)\n",
    "\n",
    "            lambda_c = 30\n",
    "            edges = np.empty([1, 3], order='C')\n",
    "            for i_node in range(cells.shape[0]):\n",
    "                # Find neighbors\n",
    "                nei = np.sum(np.isin(cell_ids, cell_ids[i_node, :]), axis=1)\n",
    "                nei_id = np.where(nei==2)\n",
    "                for i_nei in nei_id[0][:]:\n",
    "                    if i_node < i_nei:\n",
    "                        cos_theta = np.dot(normals[i_node, 0:3], normals[i_nei, 0:3])/np.linalg.norm(normals[i_node, 0:3])/np.linalg.norm(normals[i_nei, 0:3])\n",
    "                        if cos_theta >= 1.0:\n",
    "                            cos_theta = 0.9999\n",
    "                        theta = np.arccos(cos_theta)\n",
    "                        phi = np.linalg.norm(barycenters[i_node, :] - barycenters[i_nei, :])\n",
    "                        if theta > np.pi/2.0:\n",
    "                            edges = np.concatenate((edges, np.array([i_node, i_nei, -np.log10(theta/np.pi)*phi]).reshape(1, 3)), axis=0)\n",
    "                        else:\n",
    "                            beta = 1 + np.linalg.norm(np.dot(normals[i_node, 0:3], normals[i_nei, 0:3]))\n",
    "                            edges = np.concatenate((edges, np.array([i_node, i_nei, -beta*np.log10(theta/np.pi)*phi]).reshape(1, 3)), axis=0)\n",
    "            edges = np.delete(edges, 0, 0)\n",
    "            edges[:, 2] *= lambda_c*round_factor\n",
    "            edges = edges.astype(np.int32)\n",
    "\n",
    "            refine_labels = cut_from_graph(edges, unaries, pairwise)\n",
    "            refine_labels = refine_labels.reshape([-1, 1])\n",
    "\n",
    "            # output refined result\n",
    "            mesh3 = mesh_d.clone()\n",
    "            mesh3.celldata['Label']=refine_labels\n",
    "            vedo.write(mesh3, os.path.join(output_path, '{}_d_predicted_refined.vtp'.format(i_sample[:-4])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #    # upsampling\n",
    "            # print('\\tUpsampling...')\n",
    "            # if mesh.ncells > 100000:\n",
    "            #     target_num = mesh.ncells # set max number of cells\n",
    "            #     ratio = target_num/mesh.ncells # calculate ratio\n",
    "            #     mesh.decimate(fraction=ratio)\n",
    "            #     print('Original contains too many cells, simpify to {} cells'.format(mesh.ncells))\n",
    "\n",
    "            # # get fine_cells\n",
    "            # cells = np.zeros([mesh.ncells, 9], dtype='float32')\n",
    "            # for i in range(len(cells)):\n",
    "            #     cells[i][0], cells[i][1], cells[i][2] = mesh.dataset.GetPoint(mesh.dataset.GetCell(i).GetPointId(0)) # don't need to copy\n",
    "            #     cells[i][3], cells[i][4], cells[i][5] = mesh.dataset.GetPoint(mesh.dataset.GetCell(i).GetPointId(1)) # don't need to copy\n",
    "            #     cells[i][6], cells[i][7], cells[i][8] = mesh.dataset.GetPoint(mesh.dataset.GetCell(i).GetPointId(2)) # don't need to copy\n",
    "\n",
    "            # fine_cells = cells\n",
    "\n",
    "            # barycenters = mesh3.cell_centers # don't need to copy\n",
    "            # fine_barycenters = mesh.cell_centers # don't need to copy\n",
    "\n",
    "            # if upsampling_method == 'SVM':\n",
    "            #     #clf = SVC(kernel='rbf', gamma='auto', probability=True, gpu_id=gpu_id)\n",
    "            #     clf = SVC(kernel='rbf', gamma='auto', gpu_id=gpu_id)\n",
    "            #     # train SVM\n",
    "            #     #clf.fit(mesh2.cells, np.ravel(refine_labels))\n",
    "            #     #fine_labels = clf.predict(fine_cells)\n",
    "\n",
    "            #     clf.fit(barycenters, np.ravel(refine_labels))\n",
    "            #     fine_labels = clf.predict(fine_barycenters)\n",
    "            #     fine_labels = fine_labels.reshape(-1, 1)\n",
    "            # elif upsampling_method == 'KNN':\n",
    "            #     neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "            #     # train KNN\n",
    "            #     #neigh.fit(mesh2.cells, np.ravel(refine_labels))\n",
    "            #     #fine_labels = neigh.predict(fine_cells)\n",
    "\n",
    "            #     neigh.fit(barycenters, np.ravel(refine_labels))\n",
    "            #     fine_labels = neigh.predict(fine_barycenters)\n",
    "            #     fine_labels = fine_labels.reshape(-1, 1)\n",
    "\n",
    "            # mesh.celldata['Label']=fine_labels\n",
    "            # vedo.write(mesh, os.path.join(output_path, '{}_predicted_refined.vtp'.format(i_sample[:-4])))\n",
    "\n",
    "            #remove tmp folder\n",
    "            shutil.rmtree(tmp_path)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print('Sample filename: {} completed'.format(i_sample))\n",
    "            print('\\tcomputing time: {0:.2f} sec'.format(end_time-start_time))\n",
    "            return(np.unique(refine_labels))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.meshsegnet import *\n",
    "from vedo import *\n",
    "import pandas as pd\n",
    "from utils.loss_and_metrics import *\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy.io as sio\n",
    "import shutil\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pygco import cut_from_graph\n",
    "\n",
    "\n",
    "model_name ='MeshSegNetMAX_17_classes_413_best.tar'\n",
    "model_name= \"MeshSegNet_17_classes_493rotated_best.tar\"\n",
    "ip_path = './input/lower/'\n",
    "op_path = './output/lower/'\n",
    "downsample = True  # Turn to False for dental pathologies project\n",
    "p=Inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.stl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(ip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.stl\n",
      "Predicting Sample filename: 5.stl\n",
      "\tDownsampling...\n",
      "\tPredicting...\n",
      "\tRefining by pygco...\n",
      "Sample filename: 5.stl completed\n",
      "\tcomputing time: 13.34 sec\n"
     ]
    }
   ],
   "source": [
    "x= p.predict_labels(model_name, ip_path, op_path, '5.stl', downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 13, 14, 15])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered values: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example column array (9999, 1) with values from 0-16\n",
    "arr = np.random.randint(0, 17, size=(9999, 1))  # Generating random sample\n",
    "\n",
    "# Flatten the array to make it 1D\n",
    "arr_flat = arr.flatten()\n",
    "\n",
    "# Count occurrences of each unique value\n",
    "unique_values, counts = np.unique(arr_flat, return_counts=True)\n",
    "\n",
    "# Filter values that appear more than 5 times\n",
    "filtered_values = unique_values[counts > 5]\n",
    "\n",
    "print(\"Filtered values:\", filtered_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
